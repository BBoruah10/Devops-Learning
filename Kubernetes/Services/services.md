Why do we need Services in Kubernetes?

In Kubernetes:

Pods are temporary (they die and get recreated)

Every new Pod gets a new IP

You cannot rely on Pod IPs

ğŸ‘‰ Service solves this problem

âœ… What a Service does:

Gives a stable IP & DNS name

Load balances traffic across Pods

Exposes Pods inside or outside the cluster

ğŸ“Œ Simple definition

A Service is a permanent entry point to access a group of Pods.

ğŸ”¹ How Service works (very important)
User â†’ Service â†’ Pods

Service does NOT run your app

It selects Pods using labels

Traffic is distributed among matching Pods

ğŸ”¹ Things required to create a Service
1ï¸âƒ£ Pods with labels

Service finds Pods using labels.

Example Pod/Deployment label:

labels:
app: nginx

2ï¸âƒ£ Service with selector

Selector must match Pod labels.

selector:
app: nginx

3ï¸âƒ£ Port mapping

Service maps:

service port â†’ container port

ğŸ”¹ Types of Kubernetes Services (VERY IMPORTANT)
1ï¸âƒ£ ClusterIP (Default)

ğŸ“Œ Use case

Internal communication

Backend, DB, microservices

ğŸ”¹ Features

Accessible only inside cluster

Cannot be accessed from browser

Example:
kubectl expose deployment nginx-deploy \
 --name=nginx-service \
 --port=80 \
 --target-port=80

YAML:
apiVersion: v1
kind: Service
metadata:
name: nginx-service
spec:
type: ClusterIP
selector:
app: nginx
ports: - port: 80
targetPort: 80

ğŸ“Œ Access inside cluster

curl http://nginx-service

2ï¸âƒ£ NodePort

ğŸ“Œ Use case

Simple external access (testing, learning)

ğŸ”¹ Features

Exposes service on every nodeâ€™s IP

Port range: 30000â€“32767

Example:
apiVersion: v1
kind: Service
metadata:
name: nginx-nodeport
spec:
type: NodePort
selector:
app: nginx
ports: - port: 80
targetPort: 80
nodePort: 30007

Access:
http://<Node-IP>:30007

âš ï¸ Not recommended for production

3ï¸âƒ£ LoadBalancer

ğŸ“Œ Use case

Production apps

Cloud environments (AWS, GCP, Azure)

ğŸ”¹ Features

Automatically creates cloud load balancer

Provides public IP

Example:
apiVersion: v1
kind: Service
metadata:
name: nginx-lb
spec:
type: LoadBalancer
selector:
app: nginx
ports: - port: 80
targetPort: 80

Get External IP:
kubectl get svc

EXTERNAL-IP 35.190.xx.xx

4ï¸âƒ£ Headless Service (Advanced)

ğŸ“Œ Use case

Stateful apps (DBs)

Direct Pod-to-Pod communication

ğŸ”¹ Feature

No load balancing

No ClusterIP

spec:
clusterIP: None

ğŸ”¹ Important Service Concepts (Simple)
ğŸ”¸ Selector

Links Service to Pods

selector:
app: nginx

ğŸ”¸ Ports
Field Meaning
port Service port
targetPort Container port
nodePort External port
ğŸ”¸ Endpoints

Behind the scenes:

kubectl get endpoints nginx-service

Shows Pod IPs linked to Service.

ğŸ”¹ Common Commands (Must Know)

# List services

kubectl get svc

# Describe service

kubectl describe svc nginx-service

# Delete service

kubectl delete svc nginx-service

ğŸ”¹ Service vs Pod (Quick Comparison)
Pod Service
Temporary Permanent
Has changing IP Stable IP
Runs containers Routes traffic
No load balancing Load balances
ğŸ”¹ Real-life analogy ğŸ 

Pods â†’ Workers

Service â†’ Reception desk

Clients talk to reception, not workers directly

ğŸ”¹ When to use which Service?
Scenario Service
Internal microservices ClusterIP
Testing from browser NodePort
Production public app LoadBalancer
Database / Stateful app Headless
âœ… Final takeaway

âœ” Kubernetes never exposes Pods directly
âœ” Service = stable access + load balancing
âœ” Most common: ClusterIP
âœ” Production external access: LoadBalancer

# Lab

Goal of this Lab

You will:

Create a Deployment (nginx Pods)

Create a Service (expose Pods)

Access the app

See load balancing

Scale Pods and observe behavior

âœ… Prerequisites

Kubernetes cluster running (Minikube / Kind / Cloud)

kubectl configured

Check:

kubectl get nodes

ğŸ§© STEP 1: Create a Deployment

ğŸ‘‰ Deployment creates and manages Pods.

Command:
kubectl create deployment nginx-deploy --image=nginx

What just happened (simple):

Kubernetes created a Deployment

Deployment created 1 Pod

Pod runs nginx container

Check:

kubectl get deployments
kubectl get pods

ğŸ§© STEP 2: Expose Deployment as a Service (ClusterIP)

ğŸ‘‰ Now we create a Service so Pods can be accessed.

Command:
kubectl expose deployment nginx-deploy --port=80 --target-port=80

What this does:

Creates a ClusterIP Service

Connects Service â†’ Pods using labels

Gives a stable internal IP

Check:

kubectl get svc

Youâ€™ll see something like:

nginx-deploy ClusterIP 10.96.xxx.xxx 80/TCP

ğŸ§© STEP 3: Test Service from Inside Cluster

Run a temporary Pod:

kubectl run test --image=busybox -it --rm -- sh

Inside the pod:

wget -qO- http://nginx-deploy

âœ… You should see nginx welcome page HTML

ğŸ‘‰ This proves:

Service â†’ Pod â†’ Container

Exit:

exit

ğŸ§© STEP 4: Scale the Deployment (IMPORTANT)

ğŸ‘‰ Now we create multiple Pods.

kubectl scale deployment nginx-deploy --replicas=3

Check:

kubectl get pods -o wide

Youâ€™ll see 3 Pods, each with different IPs.

ğŸ§© STEP 5: Verify Load Balancing

Run again:

kubectl run test --image=busybox -it --rm -- sh

Inside:

wget -qO- http://nginx-deploy

ğŸ‘‰ Kubernetes Service:

Automatically balances traffic

Requests go to any of the 3 Pods

(Service uses round-robin-like behavior)

ğŸ§© STEP 6: Expose App Outside (NodePort)

ğŸ‘‰ Now letâ€™s access from browser.

kubectl expose deployment nginx-deploy \
 --type=NodePort \
 --port=80

Check:

kubectl get svc

Output:

nginx-deploy NodePort 10.96.xx.xx 80:30007/TCP

Access in Browser:
http://<Node-IP>:30007

User
â†“
Service (stable IP)
â†“
Endpoints
â†“
Pods (dynamic IPs)
â†“
Containers
